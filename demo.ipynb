{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c9bf72116d2c468488be41df215a517e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb57f203d6c14dcf858a22e917838a42",
              "IPY_MODEL_92a630825a664dfdae369ff1fbaa990e",
              "IPY_MODEL_eba1918bc3f24346ae3e081dec82dd8c"
            ],
            "layout": "IPY_MODEL_429d1993256c4c5ab871527452217d14"
          }
        },
        "bb57f203d6c14dcf858a22e917838a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_207d51417ab1490f8674dffc87b919a5",
            "placeholder": "​",
            "style": "IPY_MODEL_412bd3924c794689b633e6894bd54f42",
            "value": "100%"
          }
        },
        "92a630825a664dfdae369ff1fbaa990e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e0c28059892401583887281d55754fd",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff00c8b3e3444de89a265207157c1bbb",
            "value": 20
          }
        },
        "eba1918bc3f24346ae3e081dec82dd8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d74b62e762744a689d75d61d7a0baf37",
            "placeholder": "​",
            "style": "IPY_MODEL_cd8e3d765bb94eb7930d6dde1d0db741",
            "value": " 20/20 [06:12&lt;00:00, 19.50s/it]"
          }
        },
        "429d1993256c4c5ab871527452217d14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "207d51417ab1490f8674dffc87b919a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "412bd3924c794689b633e6894bd54f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e0c28059892401583887281d55754fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff00c8b3e3444de89a265207157c1bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d74b62e762744a689d75d61d7a0baf37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd8e3d765bb94eb7930d6dde1d0db741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f69662a45d04eab836444ae69c04a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_676153ef49b0468796bec8f067039491",
              "IPY_MODEL_34697ca1c62846f399eac2eaccbb1a27",
              "IPY_MODEL_d3dd5f0b67454c39a73067921b780623"
            ],
            "layout": "IPY_MODEL_3f9a8e09c00a45d194f8daf673c8ad64"
          }
        },
        "676153ef49b0468796bec8f067039491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f41ae9b1f991436eb74926a4067354dc",
            "placeholder": "​",
            "style": "IPY_MODEL_fa04d5935d224648b05bb354fe28f86f",
            "value": "100%"
          }
        },
        "34697ca1c62846f399eac2eaccbb1a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bfc5b24ee8f4a0bac324fefaf45422d",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ade0049bbca49988323f01d19d9bbf0",
            "value": 500
          }
        },
        "d3dd5f0b67454c39a73067921b780623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66637f06133247219328441d1d69ff9e",
            "placeholder": "​",
            "style": "IPY_MODEL_b1b176c76d1e46e49175e4a9a46f09ba",
            "value": " 500/500 [00:55&lt;00:00,  9.74it/s]"
          }
        },
        "3f9a8e09c00a45d194f8daf673c8ad64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f41ae9b1f991436eb74926a4067354dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa04d5935d224648b05bb354fe28f86f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bfc5b24ee8f4a0bac324fefaf45422d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ade0049bbca49988323f01d19d9bbf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66637f06133247219328441d1d69ff9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1b176c76d1e46e49175e4a9a46f09ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsXeJ1QG0h8h",
        "outputId": "d998241e-8ff8-4f10-84ff-03b95d27232a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch version: 2.5.1+cu121\n",
            "[INFO] torchvision version: 0.20.1+cu121\n",
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] running on cuda.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "print(f\"[INFO] torch version: {torch.__version__}\")\n",
        "print(f\"[INFO] torchvision version: {torchvision.__version__}\")\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary\n",
        "\n",
        "import os\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "BATCH_SIZE = 32\n",
        "SEED = 1161\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"[INFO] running on {device}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "image_path = Path(\"data/food-101-tiny\")\n",
        "file_name = \"food101tiny.zip\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "#if False:\n",
        "if image_path.is_dir():\n",
        "  print(f\"Image directory exists: {image_path}. Continue with existing data.\")\n",
        "else:\n",
        "  print(f\"Image directory not found: {image_path}\")\n",
        "  print(\"Proceed to download food-101-tiny from Kaggle...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "  urllib.request.urlretrieve(\"https://www.kaggle.com/api/v1/datasets/download/msarmi9/food101tiny\", file_name)\n",
        "  with zipfile.ZipFile(file_name, \"r\") as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "  print(\"Images created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmN1XLam5ut_",
        "outputId": "6a4cbd38-3f71-41ac-844f-cfb17ee50ae9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image directory not found: data/food-101-tiny\n",
            "Proceed to download food-101-tiny from Kaggle...\n",
            "Images created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2_model(num_classes: int=10,\n",
        "                          seed: int=1161,\n",
        "                          dropout: float=0.3):\n",
        "  \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "  Args:\n",
        "      num_classes: number of classes in the classifier head.\n",
        "          Defaults: 10 (size of food-101-tiny).\n",
        "      seed: random seed value.\n",
        "      dropout: dropout rate of the classifier head.\n",
        "\n",
        "  Returns:\n",
        "      model: EffNetB2 feature extractor model.\n",
        "      transforms: EffNetB2 image transforms.\n",
        "  \"\"\"\n",
        "\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # Freezes all layers in base model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Changes classifier head\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=dropout, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes),\n",
        "      # 1408: number of in_feature of effnetb2.classifier\n",
        "  )\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  return model.to(device), transforms"
      ],
      "metadata": {
        "id": "xVX4ekT4FuEd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2_model(seed=SEED)"
      ],
      "metadata": {
        "id": "4iM5IhiJGBe7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"valid\"\n",
        "train_data = torchvision.datasets.ImageFolder(train_dir, transform=effnetb2_transforms)\n",
        "test_data = torchvision.datasets.ImageFolder(test_dir, transform=effnetb2_transforms)\n",
        "class_names = train_data.classes\n",
        "train_dataloader_effnetb2 = DataLoader(train_data,\n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       shuffle=True,\n",
        "                                       num_workers=NUM_WORKERS,\n",
        "                                       pin_memory=True,\n",
        "                                       )\n",
        "test_dataloader_effnetb2 = DataLoader(test_data,\n",
        "                                      batch_size=BATCH_SIZE,\n",
        "                                      shuffle=False,\n",
        "                                      num_workers=NUM_WORKERS,\n",
        "                                      pin_memory=True,\n",
        "                                      )\n",
        "\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(), lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "uc-MG_NqGK8g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Trains a PyTorch model for a single epoch\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing train_loss (float), train_accuracy (float).\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X_, y_) in enumerate(dataloader):\n",
        "    # Send data to target device\n",
        "    X, y = X_.to(device), y_.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # 2. Calculate  and accumulate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate and accumulate accuracy metric across all batches\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "    # Loop through DataLoader batches\n",
        "    for batch, (X_, y_) in enumerate(dataloader):\n",
        "      # Send data to target device\n",
        "      X, y = X_.to(device), y_.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      test_pred_logits = model(X)\n",
        "\n",
        "      # 2. Calculate and accumulate loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Calculate and accumulate accuracy\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "                                       dataloader=train_dataloader,\n",
        "                                       loss_fn=loss_fn,\n",
        "                                       optimizer=optimizer,\n",
        "                                       device=device)\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "                                    dataloader=test_dataloader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    device=device)\n",
        "\n",
        "      # Print out what's happening\n",
        "    print(f\"Epoch: {epoch+1} | \"\n",
        "    f\"train_loss: {train_loss:.4f} | \"\n",
        "    f\"train_acc: {train_acc:.4f} | \"\n",
        "    f\"test_loss: {test_loss:.4f} | \"\n",
        "    f\"test_acc: {test_acc:.4f}\")\n",
        "\n",
        "      # Update results dictionary\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V6uVtVZGLHk",
        "outputId": "b985bbf4-5441-4592-a2d9-02c9d2dd369e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "import engine\n",
        "effnetb2_results = engine.train(model=effnetb2,\n",
        "                                train_dataloader=train_dataloader_effnetb2,\n",
        "                                test_dataloader=test_dataloader_effnetb2,\n",
        "                                epochs=epochs,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "c9bf72116d2c468488be41df215a517e",
            "bb57f203d6c14dcf858a22e917838a42",
            "92a630825a664dfdae369ff1fbaa990e",
            "eba1918bc3f24346ae3e081dec82dd8c",
            "429d1993256c4c5ab871527452217d14",
            "207d51417ab1490f8674dffc87b919a5",
            "412bd3924c794689b633e6894bd54f42",
            "4e0c28059892401583887281d55754fd",
            "ff00c8b3e3444de89a265207157c1bbb",
            "d74b62e762744a689d75d61d7a0baf37",
            "cd8e3d765bb94eb7930d6dde1d0db741"
          ]
        },
        "id": "z1SdCTR9QN_G",
        "outputId": "ababd39d-9c0d-4dc8-bb60-4c22cf0e8df1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9bf72116d2c468488be41df215a517e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 1.8967 | train_acc: 0.4647 | test_loss: 1.4817 | test_acc: 0.7387\n",
            "Epoch: 2 | train_loss: 1.2791 | train_acc: 0.7303 | test_loss: 1.1319 | test_acc: 0.7984\n",
            "Epoch: 3 | train_loss: 1.0115 | train_acc: 0.7706 | test_loss: 0.9496 | test_acc: 0.8199\n",
            "Epoch: 4 | train_loss: 0.8567 | train_acc: 0.8007 | test_loss: 0.8590 | test_acc: 0.8219\n",
            "Epoch: 5 | train_loss: 0.7650 | train_acc: 0.8099 | test_loss: 0.7870 | test_acc: 0.8168\n",
            "Epoch: 6 | train_loss: 0.6905 | train_acc: 0.8296 | test_loss: 0.7280 | test_acc: 0.8277\n",
            "Epoch: 7 | train_loss: 0.6637 | train_acc: 0.8304 | test_loss: 0.7003 | test_acc: 0.8375\n",
            "Epoch: 8 | train_loss: 0.6016 | train_acc: 0.8548 | test_loss: 0.6679 | test_acc: 0.8402\n",
            "Epoch: 9 | train_loss: 0.5704 | train_acc: 0.8651 | test_loss: 0.6525 | test_acc: 0.8434\n",
            "Epoch: 10 | train_loss: 0.5356 | train_acc: 0.8592 | test_loss: 0.6372 | test_acc: 0.8414\n",
            "Epoch: 11 | train_loss: 0.5068 | train_acc: 0.8720 | test_loss: 0.6271 | test_acc: 0.8453\n",
            "Epoch: 12 | train_loss: 0.4957 | train_acc: 0.8767 | test_loss: 0.6254 | test_acc: 0.8273\n",
            "Epoch: 13 | train_loss: 0.4674 | train_acc: 0.8802 | test_loss: 0.6130 | test_acc: 0.8434\n",
            "Epoch: 14 | train_loss: 0.4408 | train_acc: 0.8859 | test_loss: 0.6000 | test_acc: 0.8375\n",
            "Epoch: 15 | train_loss: 0.4400 | train_acc: 0.8903 | test_loss: 0.5970 | test_acc: 0.8395\n",
            "Epoch: 16 | train_loss: 0.4260 | train_acc: 0.8835 | test_loss: 0.5824 | test_acc: 0.8453\n",
            "Epoch: 17 | train_loss: 0.4081 | train_acc: 0.8993 | test_loss: 0.5784 | test_acc: 0.8434\n",
            "Epoch: 18 | train_loss: 0.3857 | train_acc: 0.9074 | test_loss: 0.5788 | test_acc: 0.8344\n",
            "Epoch: 19 | train_loss: 0.3907 | train_acc: 0.8959 | test_loss: 0.5720 | test_acc: 0.8395\n",
            "Epoch: 20 | train_loss: 0.3683 | train_acc: 0.8988 | test_loss: 0.5828 | test_acc: 0.8453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "target_dir_path = Path(\"models\")\n",
        "target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "model_name = \"pretrained_effnetb2_food101tiny.pth\"\n",
        "model_save_path = target_dir_path / model_name\n",
        "print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "torch.save(obj=effnetb2.state_dict(), f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kmeVrLWVmL1",
        "outputId": "9320a57a-30d9-4114-aafb-b7c791188dbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: models/pretrained_effnetb2_food101tiny.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_model(num_classes:int=3,\n",
        "                     seed:int=42):\n",
        "  \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
        "  Args:\n",
        "    num_classes (int, optional): number of target classes. Defaults to 3.\n",
        "    seed (int, optional): random seed value for output layer. Defaults to 42.\n",
        "\n",
        "  Returns:\n",
        "    model (torch.nn.Module): ViT-B/16 feature extractor model.\n",
        "    transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
        "  \"\"\"\n",
        "  # Create ViT_B_16 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "  # Freeze all layers in model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Change classifier head to suit our needs (this will be trainable)\n",
        "  torch.manual_seed(seed)\n",
        "  model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
        "                                        out_features=num_classes)) # update to reflect target number of classes\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "39gYXONZU7rA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\n",
        "def pred_and_store(paths: List[Path],\n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms,\n",
        "                   class_names: List[str],\n",
        "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
        "\n",
        "  # 2. Create an empty list to store prediction dictionaries\n",
        "  pred_list = []\n",
        "\n",
        "  # 3. Loop through target paths\n",
        "  for path in tqdm(paths):\n",
        "\n",
        "    # 4. Create empty dictionary to store prediction information for each sample\n",
        "    pred_dict = {}\n",
        "\n",
        "    # 5. Get the sample path and ground truth class name\n",
        "    pred_dict[\"image_path\"] = path\n",
        "    class_name = path.parent.stem\n",
        "    pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "    # 6. Start the prediction timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # 7. Open image path\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # 8. Transform the image, add batch dimension and put image on target device\n",
        "    transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 10. Get prediction probability, predicition label and prediction class\n",
        "    with torch.inference_mode():\n",
        "      pred_logit = model(transformed_image) # perform inference on target sample\n",
        "      pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n",
        "      pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n",
        "      pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n",
        "\n",
        "      # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)\n",
        "      pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "      pred_dict[\"pred_class\"] = pred_class\n",
        "\n",
        "      # 12. End the timer and calculate time per pred\n",
        "      end_time = timer()\n",
        "      pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
        "\n",
        "    # 13. Does the pred match the true label?\n",
        "    pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "    # 14. Add the dictionary to the list of preds\n",
        "    pred_list.append(pred_dict)\n",
        "\n",
        "  # 15. Return list of prediction dictionaries\n",
        "  return pred_list\n",
        "\n",
        "# Make predictions across test dataset with EffNetB2\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                          model=effnetb2,\n",
        "                                          transform=effnetb2_transforms,\n",
        "                                          class_names=class_names,\n",
        "                                          device=\"cpu\") # make predictions on CPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1f69662a45d04eab836444ae69c04a19",
            "676153ef49b0468796bec8f067039491",
            "34697ca1c62846f399eac2eaccbb1a27",
            "d3dd5f0b67454c39a73067921b780623",
            "3f9a8e09c00a45d194f8daf673c8ad64",
            "f41ae9b1f991436eb74926a4067354dc",
            "fa04d5935d224648b05bb354fe28f86f",
            "3bfc5b24ee8f4a0bac324fefaf45422d",
            "6ade0049bbca49988323f01d19d9bbf0",
            "66637f06133247219328441d1d69ff9e",
            "b1b176c76d1e46e49175e4a9a46f09ba"
          ]
        },
        "id": "IpmK3r1RQsG1",
        "outputId": "ad8bf6a9-f920-4ea4-acd9-6b311c5e3ad6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f69662a45d04eab836444ae69c04a19"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
        "effnetb2_test_pred_df.correct.value_counts()\n",
        "\n",
        "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"Average time per prediction: {effnetb2_average_time_per_pred} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRtWICbfYLL9",
        "outputId": "390341f8-249f-4407-de71-cd970a0aa7a7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average time per prediction: 0.1094 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import gradio as gr\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find gradio... installing it.\")\n",
        "  !pip install -q gradio\n",
        "  import gradio as gr\n",
        "\n",
        "print(f\"gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WyzKB_JrQ-T",
        "outputId": "ae416d75-f543-451c-ddf4-8e31e4e15d1a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find gradio... installing it.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hgradio version: 5.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "  \"\"\"\n",
        "  # Start the timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the target image and add a batch dimension\n",
        "  img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "  # Put model into evaluation mode and turn on inference mode\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate the prediction time\n",
        "  pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "  # Return the prediction dictionary and prediction time\n",
        "  return pred_labels_and_probs, pred_time"
      ],
      "metadata": {
        "id": "dq_trgVvzOlJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run demo with Gradio\n",
        "import random\n",
        "\n",
        "title = \"FoodVision (food-101-tiny)\"\n",
        "class_names_str = \", \".join(class_names)\n",
        "description = f\"An EfficientNetB2 feature extractor computer vision model to classify images of food. Classes: {class_names_str}.\"\n",
        "article = \"ref: [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/). Model trained with [food-101-tiny](https://www.kaggle.com/datasets/msarmi9/food101tiny).\"\n",
        "example_list = [str(filepath) for filepath in random.sample(test_data_paths, k=3)]\n",
        "\n",
        "demo = gr.Interface(fn=predict,\n",
        "                    inputs=gr.Image(type=\"pil\"),\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (s)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "demo.launch(debug=False, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "YfeC48_tyO8L",
        "outputId": "ee1cb62b-7c30-44cb-be1d-c014ebe7ffa1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://09dc1dca75d3a4bd6b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://09dc1dca75d3a4bd6b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is for deployment to Hugging face\n",
        "# URL: https://huggingface.co/spaces/rapianyo/foodvision_food101tiny\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create FoodVision mini demo path\n",
        "foodvision_food101tiny_demo_path = Path(\"demos/foodvision_food101tiny/\")\n",
        "if foodvision_food101tiny_demo_path.exists():\n",
        "  shutil.rmtree(foodvision_food101tiny_demo_path)\n",
        "  foodvision_food101tiny_demo_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "foodvision_food101tiny_examples_path = foodvision_food101tiny_demo_path / \"examples\"\n",
        "foodvision_food101tiny_examples_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Copy example_list to the examples directory\n",
        "for example in example_list:\n",
        "  example_path = Path(example)\n",
        "  destination = foodvision_food101tiny_examples_path / example_path.name\n",
        "  print(f\"[INFO] Copying {example_path} to {destination}\")\n",
        "  shutil.copy2(src=example_path, dst=destination)\n",
        "\n",
        "# Create a destination path for our target model\n",
        "effnetb2_foodvision_food101tiny_model_destination = foodvision_food101tiny_demo_path / model_name\n",
        "\n",
        "# Try to move the file\n",
        "try:\n",
        "  print(f\"[INFO] Attempting to move {model_save_path} to {effnetb2_foodvision_food101tiny_model_destination}\")\n",
        "\n",
        "  # Move the model\n",
        "  shutil.move(src=model_save_path,\n",
        "              dst=effnetb2_foodvision_food101tiny_model_destination)\n",
        "\n",
        "  print(f\"[INFO] Model move complete.\")\n",
        "\n",
        "# If the model has already been moved, check if it exists\n",
        "except:\n",
        "  print(f\"[INFO] No model found at {model_save_path}, perhaps its already been moved?\")\n",
        "  print(f\"[INFO] Model exists at {effnetb2_foodvision_food101tiny_model_destination}: {effnetb2_foodvision_food101tiny_model_destination.exists()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C21iA-hF1Dj",
        "outputId": "3c294997-5c11-4f3f-d218-37cfaf91f291"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Copying data/food-101-tiny/valid/ramen/1698372.jpg to demos/foodvision_food101tiny/examples/1698372.jpg\n",
            "[INFO] Copying data/food-101-tiny/valid/sushi/1032351.jpg to demos/foodvision_food101tiny/examples/1032351.jpg\n",
            "[INFO] Copying data/food-101-tiny/valid/sushi/1203702.jpg to demos/foodvision_food101tiny/examples/1203702.jpg\n",
            "[INFO] Attempting to move models/pretrained_effnetb2_food101tiny.pth to demos/foodvision_food101tiny/pretrained_effnetb2_food101tiny.pth\n",
            "[INFO] Model move complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write create_effnetb2_model (exactly the same as the one above)\n",
        "%%writefile demos/foodvision_food101tiny/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "def create_effnetb2_model(num_classes: int=10,\n",
        "                          seed: int=1161,\n",
        "                          dropout: float=0.3):\n",
        "  \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "  Args:\n",
        "      num_classes: number of classes in the classifier head.\n",
        "          Defaults: 10 (size of food-101-tiny).\n",
        "      seed: random seed value.\n",
        "      dropout: dropout rate of the classifier head.\n",
        "\n",
        "  Returns:\n",
        "      model: EffNetB2 feature extractor model.\n",
        "      transforms: EffNetB2 image transforms.\n",
        "  \"\"\"\n",
        "\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # Freezes all layers in base model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Changes classifier head\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=dropout, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes),\n",
        "      # 1408: number of in_feature of effnetb2.classifier\n",
        "  )\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  return model.to(device), transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFIkFyNTFhkv",
        "outputId": "86591c8b-bdf6-4f9c-c869-67fce920cb30"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_food101tiny/app.py\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "SEED = 1161\n",
        "class_names = ['apple_pie',\n",
        "               'bibimbap',\n",
        "               'cannoli',\n",
        "               'edamame',\n",
        "               'falafel',\n",
        "               'french_toast',\n",
        "               'ice_cream',\n",
        "               'ramen',\n",
        "               'sushi',\n",
        "               'tiramisu'] # food101tiny_train_data.classes\n",
        "class_names_str = \", \".join(class_names)\n",
        "model_name = \"pretrained_effnetb2_food101tiny.pth\"\n",
        "device = \"cpu\" # free on HuggingFace\n",
        "\n",
        "# Create EffNetB2 model\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(seed=SEED)\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(\n",
        "        f=model_name,\n",
        "        map_location=torch.device(device),\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create predict function\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "  \"\"\"\n",
        "  # Start the timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the target image and add a batch dimension\n",
        "  img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "  # Put model into evaluation mode and turn on inference mode\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate the prediction time\n",
        "  pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "  # Return the prediction dictionary and prediction time\n",
        "  return pred_labels_and_probs, pred_time\n",
        "\n",
        "### Create Gradio app ###\n",
        "title = \"FoodVision (food-101-tiny)\"\n",
        "description = f\"An EfficientNetB2 feature extractor computer vision model to classify images of food. Classes: {class_names_str}.\"\n",
        "article = \"ref: [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/). Model trained with [food-101-tiny](https://www.kaggle.com/datasets/msarmi9/food101tiny).\"\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "demo = gr.Interface(fn=predict,\n",
        "                    inputs=gr.Image(type=\"pil\"),\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
        "                             gr.Number(label=\"Prediction time (s)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "demo.launch(debug=False, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJE6z6xrHwem",
        "outputId": "67bbad3d-25fb-4875-da07-5293a8d1c787"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_food101tiny/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_food101tiny/requirements.txt\n",
        "torch\n",
        "torchvision\n",
        "gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa49kafQSzRA",
        "outputId": "7eda20ff-f476-44e0-9ec3-6d8674027577"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting demos/foodvision_food101tiny/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip foodvision_big folder but exclude certain files\n",
        "!cd demos/foodvision_food101tiny && zip -r ../foodvision_food101tiny.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "\n",
        "# Download the zipped FoodVision Big app (if running in Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"demos/foodvision_food101tiny.zip\")\n",
        "except:\n",
        "    print(\"Not running in Google Colab, can't use google.colab.files.download()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "rqDyh4krUzQ0",
        "outputId": "38925366-a40c-46f1-f62a-f377ae010e4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: app.py (deflated 57%)\n",
            "  adding: examples/ (stored 0%)\n",
            "  adding: examples/1698372.jpg (deflated 0%)\n",
            "  adding: examples/1203702.jpg (deflated 0%)\n",
            "  adding: examples/1032351.jpg (deflated 1%)\n",
            "  adding: model.py (deflated 55%)\n",
            "  adding: pretrained_effnetb2_food101tiny.pth (deflated 8%)\n",
            "  adding: requirements.txt (deflated 57%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_69d7196a-c1a0-4ed6-8b3c-b974cbbf5018\", \"foodvision_food101tiny.zip\", 28967650)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}